# Streams no Node

- Ler pequenas partes de um dado, antes de ler os dados por completo

## Importação de clientes via CSV (Excel)

- Situação em que um arquivo é enviado para o backend com o tamanho de 1gb e 1.000.000 de linhas

  - Caso não esteja utilizando o conceito de streams, o Node vai ler o arquivo completo, depois vai percorrer o
    arquivo e fazer cada uma das operações descritas no db
  - O problema disso é que a pessoa que esta enviando o arquivo pode ter uma internet muito lenta e isso vai demorar
    muito, pois o backend vai esperar o arquivo inteiro para conseguir ler

- Quando se utiliza o conceito de streams, os dados serão lidos aos poucos, mesmo que o arquivo inteiro ainda não tenha
  sido recebido; E nesse pouco tempo que os dados vão sendo recebidos, eles ja vão sendo tratados

## Existem 2 tipos de streams

- Readable Streams | Streams de leitura; Ex: Receber um arquivo muito grande aos poucos
- Writable Streams | Streams de escrita; Ex: Netflix, onde os dados são enviados aos poucos para o usuário

## Fundamentos

- process.stdin | Entrada de dados do usuário
- process.stdout | Saída de dados do usuário

```js
process.stdin.pipe(process.stdout)
/*
  - O pipe é um encaminhamento, ou seja, estou encaminhando tudo que eu recebo na linha de comando para a saída padrão
    Dessa forma, tudo que o usuário digitar será "duplicado"
*/
```

## Readable Stream

- Tem o método \_read() como obrigatório
- Dentro das streams eu não posso trabalhar com strings ou numbers, ele precisa estar em um buffer
- Em uma stream, diferente de um modelo de dados tradicional, eu posso trabalhar com os dados retornados mesmo antes
  de chegar no final
- O método push() é utilizado para fornecer informações para quem estiver consumindo a stream

```js
import { Readable } from "node:stream"

class OneToHundredStream extends Readable {
  index = 1

  _read() {
    const i = this.index++

    setTimeout(() => {
      if (i > 100) {
        this.push(null)
      } else {
        const buf = Buffer.from(String(i))

        this.push(buf)
      }
    }, 1000)
  }
}

new OneToHundredStream().pipe(process.stdout)
```

## Writable Stream

- Tem o método \_write() como obrigatório, e ele possui 3 parâmetros
  - chunk | Pedaço que é lido da stream de leitura | Formato Buffer
  - encoding | É como que a informação esta codificada
  - callback | Função que deve ser chamada pela stream de escrita quando ela terminou de fazer o que precisava
    fazer com aquela informação
- Dentro de uma stream de escrita, nada é retornado, pois ela vai processar o dado em alguma outra coisa

```js
import { Writable } from "node:stream"

class MultiplyByTenStream extends Writable {
  _write(chunk, encoding, callback) {
    console.log(Number(chunk.toString()) * 10)

    callback()
  }
}

new OneToHundredStream().pipe(new MultiplyByTenStream())
```

## Transform Streams

- Para transformar um dado em outro
- Precisa ler dados de algum lugar e escrever em outro
- Tem o método \_transform() como obrigatório, e possui os mesmos parâmetros do \_write()

  - A diferença é que preciso enviar alguns parâmetros no callback:

    ```js
    class InverseNumberStream extends Transform {
      _transform(chunk, encoding, callback) {
        const transformed = Number(chunk.toString()) * -1

        callback(null, transformed)
      }
    }
    /*
      - null | É o erro
        - Envio como null caso não tenha dado erro
        - Envio um new Error() caso tenha dado erro
      - transformed | É o dado transformado
    */
    ```

## Duplex Streams

- Pode conter o método de leitura e o de escrita
  - Um arquivo por exemplo, que pode ser lido e escrito algo nele

## Caso a parte

- Em alguns casos eu preciso receber todos os dados da stream antes de processar esses dados, e para isso:

  - Geralmente crio um array de buffers populando ele, e depois trabalho com esses dados

  ```js
  const server = http.createServer(async (req, res) => {
    const buffers = []

    for await (const chunk of req) {
      buffers.push(chunk)
    }

    const fullStreamContent = Buffer.concat(buffers).toString()

    console.log(fullStreamContent)

    return res.end(fullStreamContent)
  })
  ```
